{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<img src=\"https://www.th-koeln.de/img/logo.svg\" style=\"float: right;\" width=\"200\" alt=\"Th-KÃ¶ln-Logo\">"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DIS21a 1 | Big Data | Exam WS 2022/23 | Group J (10)\n",
                "\n",
                "## Participants\n",
                "\n",
                "-   [Markus Hardtke](https://elearning.iws.th-koeln.de/moodle/user/profile.php?id=2705)\n",
                "    -   Matriculation Number: 1234567\n",
                "-   [Furkan Erdogan](https://elearning.iws.th-koeln.de/moodle/user/profile.php?id=2688)\n",
                "    -   Matriculation Number: 11136112\n",
                "-   [Jannik Loose](https://elearning.iws.th-koeln.de/moodle/user/profile.php?id=2687)\n",
                "    -   Matriculation Number: 1234567\n",
                "-   [Gilles Romer](https://elearning.iws.th-koeln.de/moodle/user/profile.php?id=2681)\n",
                "    -   Matriculation Number: 11139919\n",
                "\n",
                "---\n",
                "\n",
                "## Project Setup\n",
                "\n",
                "*This Setup was made for Windows users and will differ if you are using a different OS.    \n",
                "The recommended Python version used to run this notebook is 3.9.13*\n",
                "\n",
                "1. Virtual environment\n",
                "\n",
                "    *This step is required so you can install the required packages without affecting or filling up your global Python installation.*\n",
                "\n",
                "    ```bash\n",
                "    python3 -m venv .venv\n",
                "    source .venv/Scripts/activate\n",
                "    pip install -r requirements.txt\n",
                "    ```\n",
                "\n",
                "2. Git\n",
                "\n",
                "    *This step is required so unnecessary data from the Notebook is not pushed to the repository. This will automatically remove all metadata and execution counts from the Notebook as soon as you stage your file.*\n",
                "\n",
                "    1. Download JQ (flexible command-line JSON processor) from [here](https://github.com/stedolan/jq/releases/download/jq-1.6/jq-win64.exe)\n",
                "    2. Create the following Folder: `C:\\Program Files\\jq`\n",
                "    3. Add the following line to your PATH environment variable: `C:\\Program Files\\jq`\n",
                "    4. Rename the downloaded JQ file to `jq.exe` and move it to the previously created folder\n",
                "    5. Add the following lines to your .gitconfig file (usually found in C:\\Users\\YOUR_USERNAME\\.gitconfig)\n",
                "\n",
                "        ```bash\n",
                "        [core]\n",
                "            attributesfile = ~/.gitattributes_global\n",
                "        [filter \"nbstrip_meta\"]\n",
                "            clean = \"jq --indent 4 \\\n",
                "                    '(.cells[] | select(has(\\\"execution_count\\\")) | .execution_count) = null  \\\n",
                "                    | .metadata = {\\\"language_info\\\": {\\\"name\\\": \\\"python\\\", \\\"pygments_lexer\\\": \\\"ipython3\\\"}} \\\n",
                "                    | .cells[].metadata = {} \\\n",
                "                    '\"\n",
                "            smudge = cat\n",
                "            required = true\n",
                "        [filter \"nbstrip_full\"]\n",
                "            clean = \"jq --indent 4 \\\n",
                "                    '(.cells[] | select(has(\\\"outputs\\\")) | .outputs) = []  \\\n",
                "                    | (.cells[] | select(has(\\\"execution_count\\\")) | .execution_count) = null  \\\n",
                "                    | .metadata = {\\\"language_info\\\": {\\\"name\\\": \\\"python\\\", \\\"pygments_lexer\\\": \\\"ipython3\\\"}} \\\n",
                "                    | .cells[].metadata = {} \\\n",
                "                    '\"\n",
                "            smudge = cat\n",
                "            required = true\n",
                "        ```\n",
                "\n",
                "    6. Create a `.gitattributes_global` file at the same location as your `.gitconfig` file and add the following lines:\n",
                "\n",
                "        ```bash\n",
                "        *.ipynb filter=nbstrip_meta\n",
                "        ```\n",
                "\n",
                "3. Weights and Biases\n",
                "\n",
                "    *Weights and Biases is a tool that helps you track your experiments and visualize your results. It is also used to run hyperparameter sweeps.*  \n",
                "    \n",
                "    Make sure to create an account on [Weights and Biases](https://wandb.ai/site) and accept the invitation to the Team.  \n",
                "    Furthermore you will have to add your [API key](https://wandb.ai/settings#dangerzone) to the [scrts.py](./scrts.py) file under `wandb_api_key`.\n",
                "\n",
                "## Optional Setup\n",
                "\n",
                "1. Tensorflow GPU support  \n",
                "\n",
                "    *<span style=\"color:orange\">This step is optional and only required if you want to use a GPU for training.</span>*  \n",
                "   Follow this setup to activate tensorflow GPU support (Make sure to install the mentioned versions rather than the latest one!): [https://www.tensorflow.org/install/gpu](https://www.tensorflow.org/install/gpu)  \n",
                "   This tutorial will additionally help you to install CUDA and cuDNN: [https://lifewithdata.com/2022/01/16/how-to-install-tensorflow-and-keras-with-gpu-support-on-windows/](https://lifewithdata.com/2022/01/16/how-to-install-tensorflow-and-keras-with-gpu-support-on-windows/) \n",
                "\n",
                "---\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Objective\n",
                "\n",
                "The aim of this kernel is to predict <span style=\"color:orange\">partitions of images of different environments</span> from the dataset `environments` which are `{streets, sea, mountain, glacier, forest, buildings}`. We developed a classifier that distinguishes the images in the best possible way. The biggest challenge is to get the most accuracy with using our own image classification algorithm `categorical_classification` and trying out `ResNET` as an another image classification algorithm."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Environment Setup\n",
                "\n",
                "### Path's\n",
                "\n",
                "We're importing libraries `os` and `sys`. \n",
                "The os and sys modules provide numerous tools for dealing with file names, paths and directories. We're setting up the path to the root of our repository.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Importing the necessary libraries\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# Setting up the path to the root of the repository\n",
                "path = os.getcwd()\n",
                "notebookpath = os.path.join(path, 'Exam_WS2223.ipynb')\n",
                "\n",
                "datapath = os.path.join(path, 'data')\n",
                "data_testpath = os.path.join(datapath, 'seg_test')\n",
                "data_trainpath = os.path.join(datapath, 'seg_train')\n",
                "data_valpath = os.path.join(datapath, 'seg_pred')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "### Tensorflow version and GPU availability"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We're importing here important and necessary libraries. Then we're checking with the code the versions and gpu availability to set up the GPU if it was installed in the section above. This code then will tell if the GPU is available or not in case if one should have overlooked a step.\n",
                "\n",
                "---\n",
                "\n",
                "As mentioned earlier, this section is dedicated to the optional scenario of using the GPU. The code provides information about whether a GPU is used and other information about the GPU. It also includes the Tensorflow/Keras libraries for use and shows the versions of these libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Importing the libraries\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "import gc\n",
                "\n",
                "# Checking versions and gpu availability\n",
                "print(f\"Tensorflow version: {tf.__version__}\")\n",
                "print(f\"Keras Version: {tf.keras.__version__}\")\n",
                "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
                "\n",
                "# Setting up the GPU\n",
                "gpus = tf.config.list_physical_devices('GPU')\n",
                "if gpus:\n",
                "    try:\n",
                "        # Currently, memory growth needs to be the same across GPUs\n",
                "        for gpu in gpus:\n",
                "            tf.config.experimental.set_memory_growth(gpu, True)\n",
                "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
                "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
                "    except RuntimeError as e:\n",
                "        # Memory growth must be set before GPUs have been initialized\n",
                "        print(e)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "### Wandb"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here we import more important libraries to remove unnecessary logs later.  \n",
                "**WandbMetricsLogger**: Used for Experiment Tracking.  \n",
                "**WandbModelCheckpoints**: Used to log the model checkpoints to Weight and Biases Artifacts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Importing the libraries\n",
                "import wandb\n",
                "import scrts\n",
                "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
                "import logging\n",
                "import absl.logging\n",
                "import os\n",
                "\n",
                "# Removing all unnecessary logs\n",
                "absl.logging.set_verbosity(absl.logging.ERROR)\n",
                "logger = logging.getLogger(\"wandb\")\n",
                "logger.setLevel(logging.ERROR)\n",
                "os.environ['WANDB_SILENT'] = 'true'\n",
                "os.environ['WANDB_CONSOLE'] = 'off'\n",
                "os.environ['WANDB_NOTEBOOK_NAME'] = notebookpath\n",
                "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
                "\n",
                "if scrts.wandb_api_key is None:\n",
                "    print(\"Please enter your wandb API key in scrts.py\")\n",
                "else:\n",
                "    wandb.login(key=scrts.wandb_api_key)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Helper functions\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Plotting the loss and accuracy\n",
                "We visualise the images via [seaborn](https://seaborn.pydata.org/) and [matplotlib](https://matplotlib.org/), which are afterwards split into data and labels with a defined function. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Data loading and preprocessing\n",
                "\n",
                "This code defines a list of classes and a function called `load_data`. The function loads and preprocesses images from a **\"seg_train\"** and **\"seg_test\"** directory within the provided file path. The images are resized to 150x150 and appended to a data list along with their matched folder (label). The data is then shuffled and split into images and labels for both the training and test sets. The result of the function are returned as images with labels.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Data Augmentation\n",
                "The following function applies data augmentation to the training images using the `ImageDataGenerator` class from the *TensorFlow  library*, such as random rotation or shifts, etc.. to increase the diversity of the training data. The function returns two generators, one for the training set and one for the test set, which can be used to feed images to a model during training and evaluation."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Label vectorization and image normalization\n",
                "Here we're vectorizing the labels and doing an [One-hot encoding the labels](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical). One-Hot Encoding is a popular technique for treating categorical variables. It simply creates additional features based on the number of unique values in the categorical feature. Every unique value in the category will be added as a feature.\n",
                "\n",
                "This part defines two functions, `label_vectorization` and `image_normalization`, these are used to preprocess data for machine learning. The `label_vectorization` function converts text labels to numerical labels and the `image_normalization` function converts images to arrays and normalizes them by dividing all pixel values by 255. These functions to prepare the data for the later use in the neural network model.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "# Plotting the loss and accuracy of the model\n",
                "def plot_loss_accuracy(history):\n",
                "    plt.figure(figsize=(10, 5))\n",
                "    plt.subplot(1, 2, 1)\n",
                "    plt.title(\"Loss\")\n",
                "    plt.plot(history.history[\"loss\"], label=\"Training loss\")\n",
                "    plt.plot(history.history[\"val_loss\"], label=\"Validation loss\")\n",
                "    plt.legend()\n",
                "    plt.subplot(1, 2, 2)\n",
                "    plt.title(\"Accuracy\")\n",
                "    plt.plot(history.history[\"accuracy\"], label=\"Training accuracy\")\n",
                "    plt.plot(history.history[\"val_accuracy\"], label=\"Validation accuracy\")\n",
                "    plt.legend()\n",
                "    plt.show()\n",
                "\n",
                "# Data loading and preprocessing (... a bit)\n",
                "import random\n",
                "import numpy as np\n",
                "classes = [\"buildings\", \"forest\", \"glacier\", \"mountain\", \"sea\", \"street\"]\n",
                "\n",
                "from PIL import Image\n",
                "from PIL.ExifTags import TAGS\n",
                "def load_data(datapath, classes):\n",
                "    # Only load training and test data\n",
                "    for dataset in os.listdir(datapath):\n",
                "        if dataset == \"seg_train\" or dataset == \"seg_test\":\n",
                "            # Load the data\n",
                "            data = []\n",
                "            for folder in os.listdir(os.path.join(datapath, dataset)):\n",
                "                for image in os.listdir(os.path.join(datapath, dataset, folder)):\n",
                "                    # resize images to 150x150\n",
                "                    img = Image.open(os.path.join(datapath, dataset, folder, image))\n",
                "                    img = img.resize((150, 150))\n",
                "                    data.append([img, folder])\n",
                "            # Shuffle the data\n",
                "            random.shuffle(data)\n",
                "            # Split the images into data(the actual image) and labels\n",
                "            images = []\n",
                "            labels = []\n",
                "            for image in data:\n",
                "                images.append(image[0])\n",
                "                labels.append(image[1])\n",
                "            if dataset == \"seg_train\":\n",
                "                train_images = images\n",
                "                train_labels = labels\n",
                "            else:\n",
                "                test_images = images\n",
                "                test_labels = labels\n",
                "    return train_images, train_labels, test_images, test_labels\n",
                "\n",
                "# Data augmentation\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "def augment_data(datapath, classes, train_dir=data_trainpath, test_dir=data_testpath):\n",
                "    train_datagen = ImageDataGenerator(\n",
                "        rescale=1./255,\n",
                "        rotation_range=40,\n",
                "        width_shift_range=0.2,\n",
                "        height_shift_range=0.2,\n",
                "        shear_range=0.2,\n",
                "        zoom_range=0.2,\n",
                "        horizontal_flip=True,)\n",
                "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
                "\n",
                "    train_generator = train_datagen.flow_from_directory(\n",
                "        train_dir,\n",
                "        target_size=(150, 150),\n",
                "        batch_size=20,\n",
                "        class_mode='categorical')\n",
                "    test_generator = test_datagen.flow_from_directory(\n",
                "        test_dir,\n",
                "        target_size=(150, 150),\n",
                "        batch_size=20,\n",
                "        class_mode='categorical')\n",
                "    return train_generator, test_generator\n",
                "\n",
                "# Vectorizing the labels\n",
                "def label_vectorization(labels):\n",
                "    # Convert the labels to numbers\n",
                "    for i, label in enumerate(labels):\n",
                "        labels[i] = classes.index(label)\n",
                "    # One-hot encoding the labels\n",
                "    from tensorflow.keras.utils import to_categorical\n",
                "    labels = to_categorical(labels, num_classes=len(classes))\n",
                "    return labels\n",
                "\n",
                "# Normalizing the images\n",
                "def image_normalization(data):\n",
                "    # Convert the images to arrays\n",
                "    for i, image in enumerate(data):\n",
                "        data[i] = np.array(image, dtype=\"float32\")\n",
                "    # Normalize the images\n",
                "    data = np.array(data) / 255\n",
                "    return data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## What are we working with?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### What does the data look like?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The main focus of this section is to train the data and verify if all training images have the same dimensions. A code has been implemented to accomplish these tasks and provide output via print. If there are any differences, the code will display the message. This also applies to the validation part of this code.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show metadata of images and folders\n",
                "\n",
                "# Training data\n",
                "data_train_length = sum([len(files) for r, d, files in os.walk(data_trainpath)])\n",
                "# Check if all training images have the same dimensions\n",
                "for folder in os.listdir(data_trainpath):\n",
                "    for i, image in enumerate(os.listdir(os.path.join(data_trainpath, folder))):\n",
                "        if i == 0:\n",
                "            data_train_imgsize_heigt = Image.open(os.path.join(data_trainpath, folder, image)).height\n",
                "            data_train_imgsize_width = Image.open(os.path.join(data_trainpath, folder, image)).width\n",
                "        else:\n",
                "            if data_train_imgsize_heigt != Image.open(os.path.join(data_trainpath, folder, image)).height or data_train_imgsize_width != Image.open(os.path.join(data_trainpath, folder, image)).width:\n",
                "                print(f\"The train image {folder}/{image} has different dimensions than the others\")\n",
                "                break\n",
                "print(f\"The training data contains {sum([len(files) for r, d, files in os.walk(data_trainpath)])} images with the dimensions {data_train_imgsize_heigt}x{data_train_imgsize_width} pixels\")\n",
                "\n",
                "# Test data\n",
                "data_val_length = sum([len(files) for r, d, files in os.walk(data_valpath)])\n",
                "# Check if all test images have the same dimensions\n",
                "for folder in os.listdir(data_testpath):\n",
                "    for i, image in enumerate(os.listdir(os.path.join(data_testpath, folder))):\n",
                "        if i == 0:\n",
                "            data_test_imgsize_heigt = Image.open(os.path.join(data_testpath, folder, image)).height\n",
                "            data_test_imgsize_width = Image.open(os.path.join(data_testpath, folder, image)).width\n",
                "        else:\n",
                "            if data_test_imgsize_heigt != Image.open(os.path.join(data_testpath, folder, image)).height or data_test_imgsize_width != Image.open(os.path.join(data_testpath, folder, image)).width:\n",
                "                print(f\"The test image {folder}/{image} has different dimensions than the others\")\n",
                "                break\n",
                "print(f\"The test data contains {sum([len(files) for r, d, files in os.walk(data_testpath)])} images with the dimensions {data_test_imgsize_heigt}x{data_test_imgsize_width} pixels\")\n",
                "\n",
                "# Validation data\n",
                "data_val_length = sum([len(files) for r, d, files in os.walk(data_valpath)])\n",
                "# Check if all validation images have the same dimensions\n",
                "for i, image in enumerate(os.listdir(data_valpath)):\n",
                "    if i == 0:\n",
                "        data_val_imgsize_heigt = Image.open(os.path.join(data_valpath, image)).height\n",
                "        data_val_imgsize_width = Image.open(os.path.join(data_valpath, image)).width\n",
                "    else:\n",
                "        if data_val_imgsize_heigt != Image.open(os.path.join(data_valpath, image)) or data_val_imgsize_width != Image.open(os.path.join(data_valpath, image)):\n",
                "            print(f\"The validation image {image} has different dimensions than the others\")\n",
                "            break\n",
                "print(f\"The validation data contains {sum([len(files) for r, d, files in os.walk(data_valpath)])} images with the dimensions {data_val_imgsize_heigt}x{data_val_imgsize_width} pixels\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "General image size = 150x150"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_images, train_labels, test_images, test_labels = load_data(datapath, classes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_generator, test_generator = augment_data(datapath, classes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_labels = label_vectorization(train_labels)\n",
                "test_labels = label_vectorization(test_labels)\n",
                "\n",
                "train_images = image_normalization(train_images)\n",
                "test_images = image_normalization(test_images)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "### Checking dataset structure again"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This code is checking the dimensions of the training and test dataset. The code then uses the libraries `matplotlib` and `seaborn` again to display the data in various visualizations. The visualizations show the count and proportion of each class in the training and test data. Furthermore, it also shows 5 random images from each class using the `imshow` function from `matplotlib`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Checking the dimensions of the data\n",
                "print(f\"The training data contains {len(train_images)} images with the dimensions {train_images[0].shape} pixels\")\n",
                "print(f\"The test data contains {len(test_images)} images with the dimensions {test_images[0].shape} pixels\")\n",
                "\n",
                "# Showing training and test data count for each class\n",
                "sns.set_style(\"darkgrid\")\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.title(\"Training and test data count for each class\")\n",
                "plt.xlabel(\"Class\")\n",
                "plt.ylabel(\"Count\")\n",
                "plt.bar(classes, [train_labels[:, i].sum() for i in range(len(classes))], label=\"Training data\")\n",
                "plt.bar(classes, [test_labels[:, i].sum() for i in range(len(classes))], label=\"Test data\")\n",
                "for i in range(len(classes)):\n",
                "    plt.text(x=classes[i], y=train_labels[:, i].sum(), s=train_labels[:, i].sum(), ha=\"center\")\n",
                "    plt.text(x=classes[i], y=test_labels[:, i].sum(), s=test_labels[:, i].sum(), ha=\"center\")\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "# Showing the proportion each class has in the training and test data in a pie chart\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.title(\"Training data proportions\")\n",
                "plt.pie([train_labels[:, i].sum() for i in range(len(classes))], labels=classes, autopct=\"%1.1f%%\")\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.title(\"Test data proportions\")\n",
                "plt.pie([test_labels[:, i].sum() for i in range(len(classes))], labels=classes, autopct=\"%1.1f%%\")\n",
                "plt.show()\n",
                "\n",
                "# Showing 5 random images from each class\n",
                "plt.figure(figsize=(5, 5))\n",
                "plt.suptitle(\"Image examples from each class\")\n",
                "for i in range(len(classes)):\n",
                "    plt.subplot(2, 3, i + 1)\n",
                "    plt.title(classes[i])\n",
                "    plt.axis(\"off\")\n",
                "    plt.imshow(train_images[np.random.choice(np.where(train_labels[:, i] == 1)[0])])\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Building the model\n",
                "\n",
                "Each Keras model is created using either the `Sequential` class, which is a linear stack of layers, or the Model functional class, which is more customizable. We're going to import Keras, obviously, but then also specifically the Sequential model type, `dense` layers, `dropout`, and `flatten` (to flatten the data before passing through the final, regular dense layer). Finally, we're using a convolutional neural network, so we're going to use `Conv2D` and `MaxPooling2D` for that.  Later in the code we train the model by defining a function with `batch_size`, `epochs` and `log_freq=10`.  At the end we fit the model and output `history`.\n",
                "\n",
                "In this a simple module, we coded a function to creats layers. Its structed this way to make it easier to add new layers and to test new approaches. Later in the code we train the model by defining a function with `batch_size`, `epochs` and `log_freq=10`.  At the end we fit the model and output `history`."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Plain classification model\n",
                "\n",
                "This code defines a configuration for a machine learning model using the WandB library, which allows for optimization of the model's hyperparameters. The configuration specifies that the 'val_acc' metric should be maximized, and defines a list of possible values for the number of epochs, batch size, dropout rate and optimizer to use.\n",
                "\n",
                "The function fit.model, which takes in the model, number of epochs, batch size, and training and test data, and returns the training history"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# WandB configuration for the plain model\n",
                "sweep_config = {\n",
                "        'method': 'bayes',\n",
                "        'metric': {\n",
                "                'name': 'val_acc',\n",
                "                'goal': 'maximize'\n",
                "        },\n",
                "        'parameters': {\n",
                "                'epochs': {\n",
                "                        'values': [10, 20, 100]\n",
                "                },\n",
                "                'batch_size': {\n",
                "                        'values': [32, 64, 128]\n",
                "                },\n",
                "                'dropout': {\n",
                "                        'values': [0.4, 0.5, 0.6]\n",
                "                },\n",
                "                'optimizer': {\n",
                "                        'values': ['adam', 'rmsprop']\n",
                "                },\n",
                "        }\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Importing the necessary libraries\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "\n",
                "# model builder\n",
                "def build_model(dropout, optimizer, layers=[32, 64, 128, 128], input_shape=(150, 150, 3)):\n",
                "    tf.keras.backend.clear_session()\n",
                "    gc.collect()\n",
                "    model = Sequential()\n",
                "\n",
                "    for i, layer in enumerate(layers):\n",
                "        if i == 0:\n",
                "            model.add(Conv2D(layer, (3, 3), activation=\"relu\", input_shape=input_shape))\n",
                "            model.add(MaxPooling2D((2, 2)))\n",
                "        else:\n",
                "            model.add(Conv2D(layer, (3, 3), activation=\"relu\"))\n",
                "            model.add(MaxPooling2D((2, 2)))\n",
                "\n",
                "    model.add(Flatten())\n",
                "    model.add(Dropout(dropout))\n",
                "    model.add(Dense(512, activation=\"relu\"))\n",
                "    model.add(Dense(len(classes), activation=\"softmax\"))\n",
                "\n",
                "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
                "\n",
                "    return model\n",
                "\n",
                "def fit_model(model, epochs, batch_size, train_images=train_images, train_labels=train_labels, test_images=test_images, test_labels=test_labels):\n",
                "    history = model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(test_images, test_labels), verbose=0)\n",
                "    return history"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### WandB run for auto logging"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sweep_train(config_defaults=None):\n",
                "    # Initialize wandb with a sample project name\n",
                "    with wandb.init(project=\"Exam\", entity=\"dis21a1_ws22-21_gruppe10\", config=config_defaults):  # this gets over-written in the Sweep\n",
                "\n",
                "        # Specify the other hyperparameters to the configuration, if any\n",
                "        wandb.config.architecture_name = \"Plain CNN\"\n",
                "        wandb.config.dataset_name = \"data\"\n",
                "\n",
                "        # initialize model\n",
                "        model = build_model(\n",
                "            wandb.config.dropout,\n",
                "            wandb.config.optimizer\n",
                "        )\n",
                "\n",
                "        fit_model(\n",
                "            model,\n",
                "            wandb.config.epochs,\n",
                "            wandb.config.batch_size\n",
                "        )\n",
                "\n",
                "sweep_id = wandb.sweep(sweep_config, project=\"Exam\")\n",
                "wandb.agent(sweep_id, function=sweep_train, count=10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Manual run for demonstration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# model training\n",
                "plain_model = build_model()\n",
                "# plotting the loss and accuracy of the model\n",
                "plot_loss_accuracy(fit_model(plain_model, 20, 128))\n",
                "# Evaluating the model\n",
                "plain_test_loss, plain_test_accuracy = plain_model.evaluate(test_images, test_labels)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import tabulate\n",
                "\n",
                "# Showing the WandB results in a table\n",
                "# Reading in csv file\n",
                "df = pd.read_csv(\"test.csv\")\n",
                "# Sorting the results by validation accuracy\n",
                "df = df.sort_values(by=\"val_acc\", ascending=False)\n",
                "# Showing the results in a fancy grid\n",
                "print(tabulate.tabulate(df, headers=\"keys\", tablefmt=\"fancy_grid\", showindex=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "### Resnet18 Model\n",
                "\n",
                "**ResNet**, short for ***Residual Network*** is a specific type of neural network that was introduced in 2015 by *Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun* in their paper \t[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385). **ResNet** itself is an artificial neural network that introduced a so-called âidentity shortcut connection,â which allows the model to skip one or more layers. This approach makes it possible to train the network on thousands of layers without affecting performance. Itâs become one of the most popular architectures for various computer vision tasks.\n",
                "\n",
                "- Won 1st place in the ILSVRC 2015 classification competition with a top-5 error rate of 3.57% (An ensemble model)\n",
                "- Won the 1st place in ILSVRC and COCO 2015 competition in ImageNet Detection, ImageNet localization, Coco detection and Coco segmentation.\n",
                "- Replacing VGG-16 layers in Faster R-CNN with ResNet-101. They observed relative improvements of 28%\n",
                "- Efficiently trained networks with 100 layers and 1000 layers also.\n",
                "\n",
                "\n",
                "\n",
                "The code imports the `ResNet18` model and uses it to build a new model by adding a flatten layer and two dense layers. Furthermore the model is then compiled with `categorical_crossentropy` as loss function, `rmsprop` as optimizer, and `acc` as metrics.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from classification_models.keras import Classifiers\n",
                "ResNet18, preprocess_input = Classifiers.get('resnet18')\n",
                "def build_Resnet_model():\n",
                "    tf.keras.backend.clear_session()\n",
                "    gc.collect()\n",
                "    model = Sequential()\n",
                "\n",
                "    base = ResNet18(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
                "    model.add(base)\n",
                "    model.add(Flatten())\n",
                "    model.add(Dense(256, activation='relu'))\n",
                "    model.add(Dense(len(classes), activation='sigmoid'))\n",
                "\n",
                "    model.compile(\n",
                "        loss='categorical_crossentropy',\n",
                "        optimizer = \"rmsprop\",\n",
                "        metrics=['acc']\n",
                "        )\n",
                "\n",
                "    return model\n",
                "\n",
                "# Model training\n",
                "# resnet18_model = build_Resnet_model()\n",
                "# resnet18_history = build_Resnet_model().fit(train_images, train_labels, epochs=20, batch_size=128, validation_data=(test_images, test_labels), verbose=0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plotting the loss and accuracy of the model\n",
                "# plot_loss_accuracy(resnet18_history)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluating the model\n",
                "# resnet18_test_loss, resnet18_test_accuracy = resnet18_model.evaluate(test_images, test_labels)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "### Classification model with data augmentation\n",
                "\n",
                "This is the same modul, it only differentiate in the way that we use the augmented version of the data\n",
                "\n",
                "\n",
                "The code is creating a function called `build_model` which takes in three parameters: \n",
                "- layers \n",
                "- dropout\n",
                "- input_shape. \n",
                "\n",
                "The function creates a new `Sequential` model, then adds several `Conv2D` and `MaxPooling2D` layers to the model based on the input layers parameter. Afterwards, it creates the model and trains it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# model builder\n",
                "def build_model(layers=[32, 64, 128, 128], dropout=0.5, input_shape=(150, 150, 3)):\n",
                "    tf.keras.backend.clear_session()\n",
                "    gc.collect()\n",
                "    model = Sequential()\n",
                "\n",
                "    for i, layer in enumerate(layers):\n",
                "        if i == 0:\n",
                "            model.add(Conv2D(layer, (3, 3), activation=\"relu\", input_shape=input_shape))\n",
                "            model.add(MaxPooling2D((2, 2)))\n",
                "        else:\n",
                "            model.add(Conv2D(layer, (3, 3), activation=\"relu\"))\n",
                "            model.add(MaxPooling2D((2, 2)))\n",
                "\n",
                "    model.add(Flatten())\n",
                "    model.add(Dropout(dropout))\n",
                "    model.add(Dense(512, activation=\"relu\"))\n",
                "    model.add(Dense(len(classes), activation=\"softmax\"))\n",
                "\n",
                "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
                "\n",
                "    return model\n",
                "\n",
                "# model training\n",
                "# DA_model = build_model()\n",
                "# DA_history = build_model().fit(train_generator, epochs=20, batch_size=128, validation_data=test_generator, verbose=0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plotting the loss and accuracy of the model\n",
                "# plot_loss_accuracy(DA_history)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluating the model\n",
                "# DA_test_loss, DA_test_accuracy = DA_model.evaluate(test_images, test_labels)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Error Analysis\n",
                "\n",
                "The dataset implementation is flawed because the images arent well implemented. Some images are poorly represented like the mountain and a glacier as an example. Error Analysis takes an example of a few images by visualizing and plotting them.\n",
                "\n",
                "The validated data from each data is run again to compare the predicted labels with the original labels. The mislabeled images are viewed.\n",
                "\n",
                "### Why our model cant achieve 100 percent\n",
                "The reason why our model cant go beyond 90 percent is because the images, which were given to us as training data are not clean with the labels. To achieve a higher percentage accuracy, we would have to do preprocessing on the images. We have already tried this by rotating the images etc. and by reusing the generators, but this has obviously not been enough.\n",
                "\n",
                "\n",
                "### Confusion Matrix\n",
                "Finally, the code creates a confusion matrix and displays it in a heatmap. The confusion matrix shows the number of times each label was predicted as each other label. The x-axis represents the predicted labels and the y-axis represents the true labels."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Hyperparameter tuning on best model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vector of probabilities\n",
                "predictions = plain_model.predict(test_images)\n",
                "# We take the highest probability\n",
                "pred_labels = np.argmax(predictions, axis = 1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show the mislabeled images along with their true and predicted labels\n",
                "plt.figure(figsize=(10, 10))\n",
                "plt.suptitle(\"Mislabeled images\\nTrue label - Predicted label\")\n",
                "for i, mislabeled in enumerate(np.where(pred_labels != test_labels.argmax(axis=1))[0][random.sample(range(0, len(np.where(pred_labels != test_labels.argmax(axis=1))[0])), 6)]):\n",
                "    plt.subplot(2, 3, i + 1)\n",
                "    plt.title(f\"{classes[test_labels.argmax(axis=1)[mislabeled]]} - {classes[pred_labels[mislabeled]]}\")\n",
                "    plt.axis(\"off\")\n",
                "    plt.imshow(test_images[mislabeled])\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show a confusion matrix of the mislabeled images\n",
                "confusion_matrix = tf.math.confusion_matrix(labels=test_labels.argmax(axis=1), predictions=pred_labels)\n",
                "plt.figure(figsize=(5, 5))\n",
                "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", xticklabels=classes, yticklabels=classes)\n",
                "plt.title(\"Confusion matrix of the mislabeled images\")\n",
                "plt.xlabel(\"Predicted label\")\n",
                "plt.ylabel(\"True label\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## VGG16 Model with feature extraction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from keras.applications.vgg16 import VGG16\n",
                "from keras.models import Model\n",
                "\n",
                "model = VGG16(weights='imagenet', include_top=False)\n",
                "model = Model(inputs=model.inputs, outputs=model.layers[-5].output)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_features = model.predict(train_images)\n",
                "test_features = model.predict(test_images)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from keras.layers import Input, Dense, Conv2D, Activation , MaxPooling2D, Flatten\n",
                "\n",
                "model2 = VGG16(weights='imagenet', include_top=False)\n",
                "\n",
                "input_shape = model2.layers[-4].get_input_shape_at(0) # get the input shape of desired layer\n",
                "layer_input = Input(shape = (9, 9, 512)) # a new input tensor to be able to feed the desired layer\n",
                "# https://stackoverflow.com/questions/52800025/keras-give-input-to-intermediate-layer-and-get-final-output\n",
                "\n",
                "x = layer_input\n",
                "for layer in model2.layers[-4::1]:\n",
                "    x = layer(x)\n",
                "    \n",
                "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
                "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
                "x = Flatten()(x)\n",
                "x = Dense(100,activation='relu')(x)\n",
                "x = Dense(6,activation='softmax')(x)\n",
                "\n",
                "# create the model\n",
                "new_model = Model(layer_input, x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "new_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "history = new_model.fit(train_features, train_labels, batch_size=128, epochs=5, validation_split = 0.2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plotting the loss and accuracy of the model\n",
                "plot_loss_accuracy(history)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "predictions = new_model.predict(test_features)    \n",
                "pred_labels = np.argmax(predictions, axis = 1)\n",
                "\n",
                "# print the accuracy in percent\n",
                "print(\"Accuracy: \", accuracy_score(test_labels.argmax(axis=1), pred_labels)*100, \"%\")"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python",
            "pygments_lexer": "ipython3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
